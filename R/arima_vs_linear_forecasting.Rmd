---
title: "A Forecasting Model Comparison"
author: "VickyBanks"
date: "`r Sys.Date()`"
output: html_document
toc: true
toc_float: true
ceruleancss: picture.css
---
# {.tabset .tabset-fade}
<style>
.main-container {
    max-width: 940px;
    margin-left: auto;
    margin-right: auto;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(error = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width=7, fig.height=5)
knitr::knit_hooks$get("source")
knitr::knit_hooks$get("output")
invisible(gc())
library(tidyverse)
library(dbplyr)
library(knitr)
library(ggplot2)
library(lubridate)
library(scales)
library(kableExtra)
library(zoo)

theme_set(theme_classic())


```



```{r read_in_data, echo = FALSE}

### covid factor
covid_factor<- read.csv("~/Documents/Projects/DS/vb_nations_2030_forecast/data/Covid trend data.csv")
covid_factor$week_commencing<-dmy(covid_factor$date)

covid_factor<-
  covid_factor %>% select(week_commencing, x_google_mob_retail) %>%
  rename(covid = x_google_mob_retail)

#covid_factor %>% head()

## dates
dates <- data.frame(week_commencing = seq(
  as.Date('2014-12-29' %>% ymd() + 7),
  by = "week",
  length.out = 52 * 8
)) %>%
  mutate(year = year(week_commencing),
         quarter = quarter(week_commencing),
         week = week(week_commencing)
         ) %>%
  filter(paste0(year,quarter) < '20203')

new_year <-
  dates %>%
  filter(week == 1) %>%
  select(week_commencing, year,quarter) %>%
  mutate(year_quarter = paste0(year,'-q',quarter))



## read in tv and radio data
radio<-read.csv('~/Documents/Projects/DS/vb_nations_2030_forecast/data/england_rajar_bbc.csv')
tv <-
  read.csv('~/Documents/Projects/DS/vb_nations_2030_forecast/data/england_tv.csv') %>% mutate(year_quarter = paste0(year, '_q', quarter))

tv$week_commencing<- ymd(tv$week_commencing)
tv$week = week(tv$week_commencing)

##clean
radio <- radio %>% mutate(region = 'England') %>%
  select(region, year, quarter, year_quarter, reach000s,hours000s, covid) %>%
  filter(year<=2020) %>%
  filter(!is.na(reach000s )) %>%
  right_join(dates, by = c('year', 'quarter') )
#radio %>% head()


##clean
tv <-
  tv %>% select(region, year, quarter, year_quarter, week, week_commencing, viewers) %>%
  left_join(covid_factor, by = 'week_commencing') %>%
  replace(is.na(.), 0) 

#tv %>% head()

```

<!-- ## Local BBC Radio England -->

<!-- ### RAJAR Data for Local England Radio  -->

<!-- The quarterly listening hours to BBC local radio stations in England from 2015 until Q1 2020, before recording data stopped because of Covid. -->

<!-- Visually the radio data suggest some cyclical behaviour with a higher number of hours in the winter quarters.  -->
<!-- <br> -->


<!-- ```{r radio, echo=FALSE} -->
<!-- x_dates <- -->
<!--   radio %>% group_by(year) %>% filter(quarter == 1) %>% select(week_commencing) %>% filter(week_commencing == min(week_commencing)) %>% ungroup() %>% mutate(year = year(week_commencing)) -->


<!-- ### radio Graph -->
<!-- ggplot(data = radio, -->
<!--        aes(x = week_commencing, y = hours000s))+ -->
<!--   geom_point(colour = 'red')+ -->
<!--   scale_y_continuous( -->
<!--     label = comma, -->
<!--     limits = ~ c(0, max(.x) * 1.1), -->
<!--     n.breaks = 10 -->
<!--   )+ -->
<!--   theme( -->
<!--     axis.text.x = element_text(angle = 90), -->
<!--     panel.grid.major.y = element_line(size = .1, color = "grey") , -->
<!--     panel.grid.major.x = element_blank(), -->
<!--     panel.grid.minor.x = element_blank(), -->
<!--     panel.grid.minor.y = element_blank(), -->
<!--     legend.title=element_blank(), -->
<!--     legend.position="none" -->
<!--   ) + -->
<!--   labs(title = "RAJAR England local radio listeners") + -->
<!--   xlab("year")+ -->
<!--   geom_vline( -->
<!--     xintercept = new_year$week_commencing, -->
<!--     linetype = "dashed", -->
<!--     color = "grey" -->
<!--   )+ -->
<!--   scale_x_date( -->
<!--     limits = as.Date(c(radio$week_commencing %>% min(),radio$week_commencing %>% max() )), -->
<!--     labels = date_format("%Y-%m-%d"), -->
<!--     breaks = x_dates$week_commencing, -->
<!--     sec.axis = sec_axis( -->
<!--       name = NULL, -->
<!--       trans = ~ ., -->
<!--       labels = function(x) format(as.yearqtr(x), "%Y") -->
<!--     ) -->
<!--   ) -->

<!-- # ### radio Graph -->
<!-- ggplot(data = radio, -->
<!--        aes(x = week_commencing, y = reach000s))+ -->
<!--   geom_point(colour = 'blue')+ -->
<!--   scale_y_continuous( -->
<!--     label = comma, -->
<!--     limits = ~ c(0, max(.x) * 1.1), -->
<!--     n.breaks = 10 -->
<!--   )+ -->
<!--   theme( -->
<!--     axis.text.x = element_text(angle = 90), -->
<!--     panel.grid.major.y = element_line(size = .1, color = "grey") , -->
<!--     panel.grid.major.x = element_blank(), -->
<!--     panel.grid.minor.x = element_blank(), -->
<!--     panel.grid.minor.y = element_blank(), -->
<!--     legend.title=element_blank(), -->
<!--     legend.position="none" -->
<!--   ) + -->
<!--   labs(title = "RAJAR England local radio listeners") + -->
<!--   xlab("year")+ -->
<!--   geom_vline( -->
<!--     xintercept = new_year$week_commencing, -->
<!--     linetype = "dashed", -->
<!--     color = "grey" -->
<!--   )+ -->
<!--   scale_x_date( -->
<!--     limits = as.Date(c(radio$week_commencing %>% min(),radio$week_commencing %>% max() )), -->
<!--     labels = date_format("%Y-%m-%d"), -->
<!--     breaks = x_dates$week_commencing, -->
<!--     sec.axis = sec_axis( -->
<!--       name = NULL, -->
<!--       trans = ~ ., -->
<!--       labels = function(x) format(as.yearqtr(x), "%Y") -->
<!--     ) -->
<!--   ) -->


<!-- radio %>% -->
<!--   select(year, quarter, reach000s, hours000s) %>% -->
<!--   unique() %>% -->
<!--   mutate(reach000s = comma(signif(reach000s,2)), -->
<!--          hours000s = comma(signif(hours000s,2))) %>% -->
<!--   kbl(booktabs = T, caption = "RAJAR England local radio listeners", -->
<!--       col.names = c("year", "quarter", "reach000s", "hours000s"), -->
<!--     escape = F) %>% -->
<!--   kable_styling(bootstrap_options =c("striped", "scale_down","hover")) -->

<!-- ``` -->

<!-- <br> <br> -->


## Local BBC TV

### Local TV News Endland

The BARB viewing figures for the local news 6:30 bulletin for BBC England from 2017. 

<br>

```{r tv, echo=FALSE}
x_dates <-
  tv %>% group_by(year) %>% filter(quarter == 1) %>% select(week_commencing) %>% filter(week_commencing == min(week_commencing)) %>% ungroup() %>% mutate(year = year(week_commencing))

new_year<- tv %>% group_by(year) %>% filter(week_commencing == min(week_commencing)) %>% select(week_commencing, year, quarter, year_quarter)

### radio Graph
ggplot(data = tv,
       aes(x = week_commencing, y = viewers))+
  geom_point(colour = 'red')+
  scale_y_continuous(
    label = comma,
    limits = ~ c(0, max(.x) * 1.1),
    n.breaks = 10
  )+
  theme(
    axis.text.x = element_text(angle = 90),
    panel.grid.major.y = element_line(size = .1, color = "grey") ,
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.title=element_blank(),
    legend.position="none"
  ) +
  labs(title = "BARB England local News Viewers") +
  xlab("year")+
  geom_vline(
    xintercept = new_year$week_commencing,
    linetype = "dashed",
    color = "grey"
  )+
  scale_x_date(
    limits = as.Date(c(tv$week_commencing %>% min(),tv$week_commencing %>% max() )),
    labels = date_format("%Y-%m-%d"),
    breaks = x_dates$week_commencing,
    sec.axis = sec_axis(
      name = NULL,
      trans = ~ .,
      labels = function(x) format(as.yearqtr(x), "%Y")
    )
  )
```

<br>
<br>
<br>


```{r tv_table_summary }


tv %>%
  select(year, week_commencing, viewers) %>%
  group_by(year) %>%
  summarise(viewers= mean(viewers)) %>%
  mutate(viewers = comma(signif(viewers,2))) %>%
  kbl(booktabs = T, caption = "BARB England local News Viewers",
      col.names = c("year", "average weekly viewers"),
      align = 'r',
    escape = F) %>%
  kable_styling(bootstrap_options =c("striped", "scale_down","hover"))


```
<br>
<br>


```{r tv_table}

tv %>%
  select(year, week_commencing, viewers) %>%
  unique() %>%
  mutate(viewers = comma(signif(viewers, 2))) %>%
  kbl(
    booktabs = T,
    caption = "BARB England local News Viewers",
    col.names = c("year", "week_commencing", "viewers"),
    align = "rrr",
    escape = F
  ) %>%
  kable_styling(bootstrap_options = c("striped", "scale_down", "hover")) %>%
  scroll_box(height = "800px")

```

<br>
<br>
<br>
<br>




## Linear Model

#### Overview

A linear model is the simplest method to forecast. The best fit for a linear trend line is created from the historical data and projected onto future dates. The linear model, at it's most basic, is nothing more than a line of best fit.

From the graph showing TV viewing, a few trends can be observed.

1. A general decline in viewing over time.
2. A seasonal trend with higher viewing in the winter months and lower viewing in the summer.
3. A significant boost throughout the Covid period from early 2020 until early 2021.

<br>
<br>

```{r tv_graph, echo=FALSE}

x_dates <-
tv %>% select(year, quarter, year_quarter, week_commencing, week) %>%
  rbind(
    data.frame(week_commencing = seq(
      as.Date(tv$week_commencing %>% max() %>% ymd() + 7),
      by = "week",
      length.out = 52 * 9
    )) %>%
      mutate(
        year = year(week_commencing),
        quarter = quarter(week_commencing),
        year_quarter = paste0(year, "_q", quarter),
        week = week(week_commencing)
      ) %>%
      filter(year < 2031) %>%
      select(year, quarter, year_quarter, week_commencing, week)
  ) %>% unique()

x_axis_qtr_dates<- x_dates %>%
  group_by(year, quarter, year_quarter) %>% summarise(week_commencing = min(week_commencing))

new_year <-
  x_dates %>% group_by(year) %>%
  filter(week_commencing == min(week_commencing)) %>%
  select(week_commencing, year, quarter, year_quarter)



### tv Graph
tv_graph<-
  ggplot(data = tv,
       aes(x = week_commencing, y = viewers))+
  geom_point(colour = 'red')+
  scale_y_continuous(
    label = comma,
    limits = ~ c(0, max(.x) * 1.1),
    n.breaks = 10
  )+
  theme(
    axis.text.x = element_text(angle = 90),
    panel.grid.major.y = element_line(size = .1, color = "grey") ,
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.title=element_blank(),
    legend.position="none"
  ) +
  labs(title = "BARB England local News Viewers") +
  xlab("year")+
  geom_vline(
    xintercept = new_year$week_commencing[new_year$week_commencing <= max(tv$week_commencing) ] ,
    linetype = "dashed",
    color = "grey"
  )+
  scale_x_date(
    limits = as.Date(
      c(
        x_axis_qtr_dates$week_commencing[x_axis_qtr_dates$week_commencing <= max(tv$week_commencing)] %>% min(),
        x_axis_qtr_dates$week_commencing[x_axis_qtr_dates$week_commencing <= max(tv$week_commencing)] %>% max()
      )
    ),
    labels =  function(x)
      format(as.yearqtr(x), "%Y Q%q'"),
    breaks = x_axis_qtr_dates$week_commencing[x_axis_qtr_dates$quarter == 1 & 
                                       x_axis_qtr_dates$week_commencing <= max(tv$week_commencing)] ,
sec.axis = sec_axis(
  name = NULL,
  trans = ~ .,
  labels = function(x)
    format(as.yearqtr(x), "%Y")
)
  )
tv_graph
```

### Simple trend 

<br>
Adding a simple linear trend to the graph produces the forecast shown below in black where a downwards trend is given. This trend has no seasonality and looks to be inflated by the higher than typical viewing during Covid. A blue trend line was also added which uses historic data from pre-Covid and shows a much steeper downwards trend. 

<br>
A rhyming poem about a work meeting where one colleague is evasive and refuses to share information when asked saying they are over works and git hub isn’t part of their job. They won’t do anyone any favours from this day forward.

```{r tv_simple_forecast, echo=FALSE}


### tv Graph
tv_basic_graph <-
  ggplot(data = tv,
       aes(x = week_commencing, y = viewers))+
  geom_point(colour = 'red')+
  scale_y_continuous(
    label = comma,
    limits = ~ c(0, max(.x) * 1.1),
    n.breaks = 10
  )+
  theme(
    axis.text.x = element_text(angle = 90),
    panel.grid.major.y = element_line(size = .1, color = "grey") ,
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    legend.title=element_blank(),
    legend.position="none"
  ) +
  labs(title = "BARB England local News Viewers") +
  xlab("year")+
  geom_vline(
    xintercept = new_year$week_commencing ,
    linetype = "dashed",
    color = "grey"
  )+
  scale_x_date(
    limits = as.Date(
      c(
        x_axis_qtr_dates$week_commencing%>% min(),
        x_axis_qtr_dates$week_commencing%>% max()
      )
    ),
    labels =  function(x)
      format(as.yearqtr(x), "%Y Q%q'"),
    breaks = x_axis_qtr_dates$week_commencing[x_axis_qtr_dates$quarter == 1 ] ,
sec.axis = sec_axis(
  name = NULL,
  trans = ~ .,
  labels = function(x)
    format(as.yearqtr(x), "%Y")
)
  )

tv_basic_graph +
  geom_smooth(
    method = "lm",
    colour = "black",
    linetype = "dashed",
    fullrange = TRUE,
    se = FALSE
  ) +
  geom_smooth(
    data = tv %>% filter(year < 2020),
    method = "lm",
    colour = "blue",
    linetype = "dashed",
    fullrange = TRUE,
    se = FALSE
  )

```

<br>

The linear model so far is of the form `y ~ x` where y is the number of viewers and x is the week commencing. However the trends identified visually are not dependent on the factor 'week commencing' itself.

1. The downwards trend is linked to the year of viewing.
2. The seasonal viewing is linked to the quarter as there is lower viewing in the summer quarter and higher in the winter quarters.
<br>

Plotted on the graph are the linear models giving:

* `viewers ~ year` in red, 
* `viewers ~ quarter` in green
* `viewers ~ week` in purple



Using just the year gives the gradual decline over all we would expect, whilst using just the quarter or just the week shows a seasonal variation. However, the seasonal trend identifiable shows lower viewing in the summer months and higher viewing in the winter, but both the quarter and week linear model show the highest point at new year and a gradual decline throughout the year which is not correct. 


```{r tv_make_trends, echo=FALSE}

model_year <- lm(data = tv   , formula = viewers ~ year)
future_dates<- x_dates %>% filter(week_commencing > max(tv$week_commencing))
forecast_year <-
  cbind(future_dates) %>%
  data.frame(viewers =  predict(model_year, future_dates)) 


model_qtr <- lm(data = tv, formula = viewers ~ quarter)
future_dates<- x_dates %>% filter(week_commencing > max(tv$week_commencing))
forecast_qtr <-
  cbind(future_dates) %>%
  data.frame(viewers =  predict(model_qtr, future_dates))


model_week <- lm(data = tv, formula = viewers ~ week)
future_dates<- x_dates %>% filter(week_commencing > max(tv$week_commencing))
forecast_week <-
  cbind(future_dates) %>%
  data.frame(viewers =  predict(model_week, future_dates))



```

```{r tv_add_group_trends, echo=FALSE}

tv_basic_graph +
  geom_smooth(
    method = "lm",
    colour = "black",
    linetype = "dashed",
    fullrange = TRUE,
    se = FALSE
  ) +
  geom_smooth(
    data = tv %>% filter(year < 2020),
    method = "lm",
    colour = "blue",
    linetype = "dashed",
    fullrange = TRUE,
    se = FALSE
  ) +
   geom_line(
    data = forecast_year,
    size = 1.35,
    colour = "red",
    #linetype = "dashed",
    fullrange = TRUE
  )+
  geom_line(
    data = forecast_qtr,
    size = 1.00,
    method = "lm",
    colour = "green",
    #linetype = "dashed",
    fullrange = TRUE
  ) +
    geom_line(
    data = forecast_week,
    size = 1.00,
    method = "lm",
    colour = "purple",
    #linetype = "dashed",
    fullrange = TRUE
  ) 



```
<br>

One way to artificially create the seasonal trend is to add an x^3^ term to the year based model. This would take the form `viewers ~ year + poly(week,3)` and gives the desired seasonal trend. 

A final step would be to include some dummy data to show that Covid is an inflated period: here the data is the Google mobility trends where values during the Covid period are negative, to decrease traffic, and outside of the Covid period they're zero. This would be included in the model in the form `viewers ~ year + poly(week,3) + covid` and is plotted on the graph in red.

This shows the seasonal trend as desired, the gradual yearly trend, and understands that the rise from Covid is not an indicator of future viewer numbers. 

However, using the linear model in this way relied on a visual interpretation of the trend, particularly the seasonal one, rather than a statistical look at the data and a forecast model based upon this.

<br>

```{r tv_add_trends, echo=FALSE}

model_year_poly_week <- lm(data = tv, formula = viewers ~ year+ poly(week,3) + covid)
future_dates<- x_dates %>% filter(week_commencing > max(tv$week_commencing)) %>% mutate(covid = 0)
forecast_year_poly_week <-
  cbind(future_dates) %>%
  data.frame(viewers =  predict(model_year_poly_week, future_dates)) #%>%



tv_basic_graph +
  # geom_smooth(
  #   method = "lm",
  #   colour = "black",
  #   linetype = "dashed",
  #   fullrange = TRUE,
  #   se = FALSE
  # ) +
  # geom_smooth(
  #   data = tv %>% filter(year < 2020),
  #   method = "lm",
  #   colour = "blue",
  #   linetype = "dashed",
  #   fullrange = TRUE,
  #   se = FALSE
  # ) +
  geom_line(
    data = forecast_year_poly_week,
    size = 1.25,
    method = "lm",
    colour = "red",
    fullrange = TRUE
  ) 



```

<br>

```{r tv_forecast, echo = FALSE}
forecast_year_poly_week %>% 
  group_by(year, quarter) %>% 
  summarise(weekly_viewers = comma(signif(mean(viewers),2))) %>% 
  filter(paste0(year,quarter)>'20223' ) %>% 
  kbl(booktabs = T, caption = "Forecast wiewers numbers for England local News",
      col.names = c("year", "quarter", " average weekly viewers"),
    escape = F) %>%
  kable_styling(bootstrap_options =c("striped", "scale_down","hover"))

```

<br>
<br>
<br> 

## ARIMA Modelling

### ARIMA Modelling

Generally, any time series can be written as a series of three components: a trend, a seasonal component and a random component.

**A**uto-**R**egressive **I**ntegrated **M**oving **A**verage is a more complex forecasting technique that looks at auto-correlations and moving averages, comparing the data at one period in time to the previous period.  


The ARIMA model is mathematically of the form 

$$ y_t =  \delta + \phi_1 y_{t-1} + ... \phi_1 y_{t-p} + \notag\\
\theta_1 \epsilon_{t-1} + ... \theta_1 \epsilon_{t-q} + \epsilon_t$$

The terms given on the top line make the auto-regressive part of the formula where the components dependent on previous values of $y$ at given times $t$ multiplied by a scale factor $\phi$. The previous $p$ number of terms are used. For example if `p = 2` then the value of y at the previous time $y_{t-1}$ and the previous time $y_{t-2}$ would be included. 

The terms on the bottom line make the moving average part of the formula, where the difference between the previous value of y ($y_{t-1}$) and the moving average is given as $\epsilon_{t-1}$ multiplied by a scale factor $\theta$. Similar to the auto-regressive part, the previous $q$ terms are used. 


The final terms $\delta$ and $\epsilon_{t}$ are essentially a white noise component with $\delta$ a constant and $\epsilon_{t}$ an error term for the given time.


The model is written in R in the form: 

```
forecast::Arima(
  time_series, 
  order  = c(p,d,q),
  seasonal = list(order = c(P,D,Q), period = n),
             include.drift = TRUE
             )
```
where the terms given are (in brief)

* data as a time series object
* order terms for non-seasonal part of the data
* seasonal terms for a given period (i.e. n = 52 for weekly)
* 'p' is the number of terms to lag for in the auto-regressive component
* 'd' is the number of times a differencing is applied
* 'q' is the lag for the moving average

<br>

### Determining the components for the ARIMA Model

<br>

#### Converting to a time series

The TV data is very simply converted to a time series. 

```{r convert_to_ts, echo = TRUE}
library(tseries);library(zoo);library(forecast)

## the data frame
tv %>% head()

# Convert to time series
data_ts<-ts(tv$viewers, 
            freq= 365.25/7, #to get weeks in a year 
            start=decimal_date(tv$week_commencing %>% min())
)
data_ts %>% head()

## plot the data to show it's still of the same shape
plot(data_ts)

```


<br>

As discussed in the linear model section, a long term trend and a seasonal trend can both be identified visually but, whereas the linear model approximated relationships and added a simple x^3^ term to make the season trend appear correct, the ARIMA model uses statistical techniques to determine what trends are truly in the data.

Decomposing the series is a good way to gain an initial understanding.

```{r decompose, echo = TRUE}
plot(decompose(data_ts))
```

The trend section of the data gives the biggest cause for concern as there is a downwards trend with a clear rise during the Covid dates. We know that rise is artificial and will not predict future trends so this suggests we may have to manipulate the data in advance to remove that. The season trend is clearly identified and shows the trend we know to be true of a rise in the winter months and a drop in the summer. The final section was automatically identified as being random fluctuations. Here the Covid rise is clearly noticeable. 



### Creating a stationary series

A stationary series is one where the observed value does not depend on the time and neither do the statistical properties such as mean or variance. 

In a stationary series visually you would be able to see fluctuation from day to day, but no seasonality and no trend over time. To check this statistically the Augmented Dickey Fuller (ADF) test is used. If the p-value is less than 0.05 there is 95% confidence in the null hypothesis and the series is stationary.


```{r stationary_stats, echo=TRUE}
## test for stationarity
adf.test(data_ts)

```

The p-value is not below 0.05 so the data is not stationary. 

Methods to make it stationary must then be used. 
The most common way to do this is to take the difference between one observation and the next, then test again for stationarity. If that series is not stationary, the difference may be taken again. The log of the values is also often taken to reduce the effect of extreme variation in the size of values. 

The function `ndiffs()` gives a suggested number of time the differencing is done.

```{r take_diff, echo = TRUE}
## What is the suggested differencing?
ndiffs(data_ts)

## What does this look like?
plot(data_ts %>% log %>%  diff())
abline(h=0, col="red")
  
## Test for stationarity
adf.test(data_ts %>%log %>%  diff())


```

The p-value is now lower than the 0.05 threshold so the null hypothesis can be accepted and the data series deemed stationary. 
As the data was differenced once to achieve this stationarity the ARIMA order value `d` takes the value one. 

<br>
<br>

### Seasonality and Cyclicity

Visual observations and the decomposition plot clearly show seasonality and cyclicity so the auto-regressive and moving average parts will need be be identified. 

To determine the values for the order terms `p` and `q` an auto-correlation plot (ACF) and a partial auto-correlation plot (PACF) are plotted. The ACF is created by looking at the correlation of a term with the term once prior, then the term twice prior, then three times and so on. Below is an example of how this is done manually and with one simple line of code. 

```{r acf_decomposition, echo = TRUE}
## create the differenced data to mimic the differenced time series data
x<-tv %>% select(viewers) %>% mutate(diff = log(viewers) - log(lag(viewers))) %>% tail(-1)
head(x)

## create three data sets with the actual data, one lag, and a second lag
x_0 <- x$diff  %>% head(-1)
x_1 <- x$diff %>% tail(-1)
x_2 <- x$diff %>% tail(-2)
## to illustrate the lags
cbind(x_0,x_1,x_2) %>% head()

## look at how correlated one lag is
plot(x_0, x_1)
abline(reg=lm(x_0 ~ x_1)) 
text(0.2,0.3, paste0('cor = ',round(cor(x_0,x_1),2)))

## look at how correlated two lag is
plot(x_0 %>% head(-1), x_2)
abline(reg=lm(x_0%>% head(-1) ~ x_2)) 
text(0.2,0.3, paste0('cor = ',round(cor(x_0 %>% head(-1),x_2),2)))
```

The correlations for the first two logs that were done manually are given below. Comparatively, the ACF plot finds the correlations for many lags and plots them.  
 
```{r acf_plot, echo = TRUE} 

## the correlation coeficients for lags 1 and 2
print(signif(cor(x_0,x_1),2) )
print(signif(cor(x_0 %>% head(-1),x_2),2))

## the ACF will find each coefficient and then plot the results. 
acf(x$diff, plot = FALSE) %>% head(n=6)
acf(x$diff, plot = TRUE )

# Using the time series gives exactly the same result
#acf(data_ts %>%log %>%  diff())
```

The correlation with lag one data is significant as it falls outside the threshold shown by the blue lines. This means the order value would be `q = 1`. 


The second technique uses a partial auto-correlation technique where the correlation is considered but any related confounding variables are removed. The PACF function is given below, and as with the ACF, as the lag 1 term is above the significance threshold the order factor is determined to be `p = 1`. 

```{r pacf, echo = TRUE}
pacf(x$diff, plot = FALSE) %>% head(n=6)
pacf(x$diff )

#pacf(data_ts %>%log %>%  diff()) equivalent for the time series data

```

<br>


### Non-Seasonal ARIMA

From the test for stationarity, the ACF, and the PACF the order values for the ARIMA were determined:

* `p = 1`
* `d = 1`
* `q = 1`

An ARIMA model can then be used to forecast the data and provides a series of coefficients. 

```{r make_non_seasonal_arima, echo=TRUE}

fit <- forecast::Arima(data_ts %>% log(), 
                       order  = c(1,1,1),
                        method = 'CSS')
fit
```

The general equation 
$$ y_t =  \delta + \phi_1 y_{t-1} + ... \phi_1 y_{t-p} + \notag\\
\theta_1 \epsilon_{t-1} + ... \theta_1 \epsilon_{t-q} + \epsilon_t$$

can be re-written as 
$$ y_t -y_{t-1}  =  \delta + 0.0633 y_{t-1} + \notag\\
-0.5094 \epsilon_{t-1} + \epsilon_t$$

The $y_t -y_{t-1}$ term is because the differenced data is being used and the coefficients are given by the model.

<br>

#### Notes for Andy

```{r mimic_arima, echo = TRUE}

## attempt to mimic the process of the ARIMA model
data.frame(y = data_ts %>% log() %>% as.numeric() ) %>%
  mutate(lag_y = lag(y), # the previous value of y
         lag_err = lag(y) - lag(y, n=2), #calculating the average of the two prior values
         AR = fit[["model"]][["phi"]]*lag_y, ## autoregression part
         MA = fit[["model"]][["theta"]] * lag_err, ## moving average part
          pred = lag_y + AR + MA
         ) %>% 
  cbind(data.frame(fitted = fit[["fitted"]]) ) %>% head(n=10)


```


This mimic does not include the $\delta$ term or the $\epsilon_{t}$ term. 


The equation given at the top is what i think it is from reading, and looking at a course I went on.


* $\delta + \epsilon_t$ - this part is a white noise component, some stable value + random error

* $\phi_1 y_{t-1} + ... \phi_1 y_{t-p}$ this bit is the AR part looking at the prior values, and for our case p= 1.

* $\theta_1 \epsilon_{t-1} + ... \theta_1 \epsilon_{t-q}$  is the MA part, but I'm not exactly sure over how many previous terms the average is calculated. (our case has q=1)


The AR part I think seems ok as it uses the previous y value and the coefficient is given in the model.

The MA part i think uses the average of the two prior terms, but i'm not sure about this. 
I read that some models give a random starting point for the first value which would then affect the error term that goes in the model.

I also don't get how you can have $\epsilon_{t}$ when you're looking at time t.

.. and this is where i did a lot of reading and think i just did move thinking.
<br>

### Evaluating the model

The residuals from the fit should appear as white noise. 

```{r residuals, echo  = TRUE}
checkresiduals(fit)
pred <- predict(fit, n.ahead = 52*9)

#ts.plot(2.718^pred$pred, log = "y", lty = c(1,3), xlab="time", ylab = "viewers (mil)")
ts.plot(data_ts , 2.718^pred$pred, log = "y", lty = c(1,3), xlab="time",ylab = "viewers (mil)")

x<-data.frame(log_data = data_ts %>% log())

x %>% head()  %>%
  mutate(previous_y = lag(log_data)) %>%
  mutate(diff = log_data - previous_y) %>%
  mutate(new_diff = 1 + 0.0633 * diff - 0.5095 * diff) %>%
  mutate(new_y = previous_y + new_diff) %>%
  cbind(data.frame(fitted = fit[["fitted"]]) %>% head()) %>%
  mutate(missing = new_y - fitted)

cbind(x = fit[["x"]],
      fitted = fit[["fitted"]],
      residuals = fit[["residuals"]]) %>% data.frame() %>%
  mutate(diff = x - fitted) %>%
  head()




```


$$ y_t =  \delta + 0.0633 y_{t-1} \notag\\
\epsilon + -0.5094 y_{t-1} $$

<!-- ## generally the value of y at time t is given by at least 3 factors -->
<!-- # the previous value of y (t-1) multiplied by some scale factor 'a'  -->
<!-- # a value 'b' dependent on time t  i.e. 'bt' -->
<!-- # a constant value 'c' -->
<!-- ## y(t) = c + bt + a y(t-1) -->






An example of a stationary series is the change in Google stock price from one day to the next. The graph appears visually stationary about the mean of zero, but the Augmented Dickey-Fuller Test can be used to statistically test for stationarity.

<!-- ```{r stationary, echo=TRUE} -->
<!-- library(quantmod) -->
<!-- getSymbols("GOOGL", src = "yahoo") -->
<!-- GOOGL <- dplyr::select(data.frame(date = index(GOOGL), coredata(GOOGL)), c('date','GOOGL.Close'))  -->

<!-- ## get google data and clean -->
<!-- GOOGL<- data.frame(date = seq(as.Date(GOOGL$date %>% min() %>% ymd()),by = "day",length.out = 5805)) %>% ##get all days -->
<!--   left_join(GOOGL, by = 'date') %>% ## join in google data -->
<!--   fill(GOOGL.Close, .direction = "down") %>%  ### set the saturday/sunday price to previous working day -->
<!--   mutate( GOOGL.Close = log(GOOGL.Close)) ## take the log -->

<!-- ## turn to time series -->
<!-- GOOGL<- ts(GOOGL$GOOGL.Close, -->
<!--             freq= 365.25, #to get weeks in a year -->
<!--             start=decimal_date(GOOGL$date %>% min()) )%>% diff() -->

<!-- plot(GOOGL) -->


<!-- ### use the ADF test  -->
<!-- adf.test(x=GOOGL) -->

<!-- ``` -->


<br>
The ADF test has a null hypothesis that there is a time dependent relationship and an alternative hypothesis that there is not, i.e the series is stationary. For this data the p-value is 0.01 which is below the 0.05 (5%) value showing that the result is statistically valid and the null hypotesis should be rejected and the alternative accepted. The data is stationary.




